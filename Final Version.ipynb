{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CÃ³pia de Color Detection.ipynb","provenance":[{"file_id":"1pcv4827T6F2SCUB7bpjkjLNlrTFCfIDD","timestamp":1635197219340}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JXjOVWVSWFAI"},"source":["IMPORTS\n"]},{"cell_type":"code","metadata":{"id":"Unt9_IPGWDIy"},"source":["# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n","\"\"\"\n","Run inference on images, videos, directories, streams, etc.\n","\n","Usage:\n","    $ python path/to/detect.py --source path/to/img.jpg --weights yolov5s.pt --img 640\n","\"\"\"\n","\n","import argparse\n","import os\n","import sys\n","from pathlib import Path\n","\n","import cv2\n","import numpy as np\n","import torch\n","import torch.backends.cudnn as cudnn\n","\n","FILE = Path(__file__).resolve()\n","ROOT = FILE.parents[0]  # YOLOv5 root directory\n","if str(ROOT) not in sys.path:\n","    sys.path.append(str(ROOT))  # add ROOT to PATH\n","ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n","\n","from models.experimental import attempt_load\n","from utils.datasets import LoadImages, LoadStreams\n","from utils.general import apply_classifier, check_img_size, check_imshow, check_requirements, check_suffix, colorstr, \\\n","    increment_path, non_max_suppression, print_args, save_one_box, scale_coords, set_logging, \\\n","    strip_optimizer, xyxy2xywh\n","from utils.plots import Annotator, colors\n","from utils.torch_utils import load_classifier, select_device, time_sync"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J0jkfbzcV83a"},"source":["# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n","\"\"\"\n","Run inference on images, videos, directories, streams, etc.\n","\n","Usage:\n","    $ python path/to/detect.py --source path/to/img.jpg --weights yolov5s.pt --img 640\n","\"\"\"\n","\n","import argparse\n","import os\n","import sys\n","from pathlib import Path\n","\n","import cv2\n","import numpy as np\n","import torch\n","import torch.backends.cudnn as cudnn\n","\n","FILE = Path(__file__).resolve()\n","ROOT = FILE.parents[0]  # YOLOv5 root directory\n","if str(ROOT) not in sys.path:\n","    sys.path.append(str(ROOT))  # add ROOT to PATH\n","ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n","\n","from models.experimental import attempt_load\n","from utils.datasets import LoadImages, LoadStreams\n","from utils.general import apply_classifier, check_img_size, check_imshow, check_requirements, check_suffix, colorstr, \\\n","    increment_path, non_max_suppression, print_args, save_one_box, scale_coords, set_logging, \\\n","    strip_optimizer, xyxy2xywh\n","from utils.plots import Annotator, colors\n","from utils.torch_utils import load_classifier, select_device, time_sync\n","\n","\n","@torch.no_grad()\n","def run(weights=ROOT / 'yolov5s.pt',  # model.pt path(s)\n","        source=ROOT / 'data/images',  # file/dir/URL/glob, 0 for webcam\n","        imgsz=640,  # inference size (pixels)\n","        conf_thres=0.25,  # confidence threshold\n","        iou_thres=0.45,  # NMS IOU threshold\n","        max_det=1000,  # maximum detections per image\n","        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n","        view_img=False,  # show results\n","        save_txt=False,  # save results to *.txt\n","        save_conf=False,  # save confidences in --save-txt labels\n","        save_crop=False,  # save cropped prediction boxes\n","        nosave=False,  # do not save images/videos\n","        classes=None,  # filter by class: --class 0, or --class 0 2 3\n","        agnostic_nms=False,  # class-agnostic NMS\n","        augment=False,  # augmented inference\n","        visualize=False,  # visualize features\n","        update=False,  # update all models\n","        project=ROOT / 'runs/detect',  # save results to project/name\n","        name='exp',  # save results to project/name\n","        exist_ok=False,  # existing project/name ok, do not increment\n","        line_thickness=3,  # bounding box thickness (pixels)\n","        hide_labels=False,  # hide labels\n","        hide_conf=False,  # hide confidences\n","        half=False,  # use FP16 half-precision inference\n","        dnn=False,  # use OpenCV DNN for ONNX inference\n","        ):\n","    source = str(source)\n","    save_img = not nosave and not source.endswith('.txt')  # save inference images\n","    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n","        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n","\n","    # Directories\n","    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n","    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n","\n","    # Initialize\n","    set_logging()\n","    device = select_device(device)\n","    half &= device.type != 'cpu'  # half precision only supported on CUDA\n","\n","    # Load model\n","    w = str(weights[0] if isinstance(weights, list) else weights)\n","    classify, suffix, suffixes = False, Path(w).suffix.lower(), ['.pt', '.onnx', '.tflite', '.pb', '']\n","    check_suffix(w, suffixes)  # check weights have acceptable suffix\n","    pt, onnx, tflite, pb, saved_model = (suffix == x for x in suffixes)  # backend booleans\n","    stride, names = 64, [f'class{i}' for i in range(1000)]  # assign defaults\n","    if pt:\n","        model = torch.jit.load(w) if 'torchscript' in w else attempt_load(weights, map_location=device)\n","        stride = int(model.stride.max())  # model stride\n","        names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n","        if half:\n","            model.half()  # to FP16\n","        if classify:  # second-stage classifier\n","            modelc = load_classifier(name='resnet50', n=2)  # initialize\n","            modelc.load_state_dict(torch.load('resnet50.pt', map_location=device)['model']).to(device).eval()\n","    elif onnx:\n","        if dnn:\n","            # check_requirements(('opencv-python>=4.5.4',))\n","            net = cv2.dnn.readNetFromONNX(w)\n","        else:\n","            check_requirements(('onnx', 'onnxruntime-gpu' if torch.has_cuda else 'onnxruntime'))\n","            import onnxruntime\n","            session = onnxruntime.InferenceSession(w, None)\n","    else:  # TensorFlow models\n","        check_requirements(('tensorflow>=2.4.1',))\n","        import tensorflow as tf\n","        if pb:  # https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt\n","            def wrap_frozen_graph(gd, inputs, outputs):\n","                x = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=\"\"), [])  # wrapped import\n","                return x.prune(tf.nest.map_structure(x.graph.as_graph_element, inputs),\n","                               tf.nest.map_structure(x.graph.as_graph_element, outputs))\n","\n","            graph_def = tf.Graph().as_graph_def()\n","            graph_def.ParseFromString(open(w, 'rb').read())\n","            frozen_func = wrap_frozen_graph(gd=graph_def, inputs=\"x:0\", outputs=\"Identity:0\")\n","        elif saved_model:\n","            model = tf.keras.models.load_model(w)\n","        elif tflite:\n","            interpreter = tf.lite.Interpreter(model_path=w)  # load TFLite model\n","            interpreter.allocate_tensors()  # allocate\n","            input_details = interpreter.get_input_details()  # inputs\n","            output_details = interpreter.get_output_details()  # outputs\n","            int8 = input_details[0]['dtype'] == np.uint8  # is TFLite quantized uint8 model\n","    imgsz = check_img_size(imgsz, s=stride)  # check image size\n","\n","    # Dataloader\n","    if webcam:\n","        view_img = check_imshow()\n","        cudnn.benchmark = True  # set True to speed up constant image size inference\n","        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt)\n","        bs = len(dataset)  # batch_size\n","    else:\n","        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)\n","        bs = 1  # batch_size\n","    vid_path, vid_writer = [None] * bs, [None] * bs\n","\n","    # Run inference\n","    if pt and device.type != 'cpu':\n","        model(torch.zeros(1, 3, *imgsz).to(device).type_as(next(model.parameters())))  # run once\n","    dt, seen = [0.0, 0.0, 0.0], 0\n","    for path, img, im0s, vid_cap in dataset:\n","        t1 = time_sync()\n","        if onnx:\n","            img = img.astype('float32')\n","        else:\n","            img = torch.from_numpy(img).to(device)\n","            img = img.half() if half else img.float()  # uint8 to fp16/32\n","        img = img / 255.0  # 0 - 255 to 0.0 - 1.0\n","        if len(img.shape) == 3:\n","            img = img[None]  # expand for batch dim\n","        t2 = time_sync()\n","        dt[0] += t2 - t1\n","\n","        # Inference\n","        if pt:\n","            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n","            pred = model(img, augment=augment, visualize=visualize)[0]\n","        elif onnx:\n","            if dnn:\n","                net.setInput(img)\n","                pred = torch.tensor(net.forward())\n","            else:\n","                pred = torch.tensor(session.run([session.get_outputs()[0].name], {session.get_inputs()[0].name: img}))\n","        else:  # tensorflow model (tflite, pb, saved_model)\n","            imn = img.permute(0, 2, 3, 1).cpu().numpy()  # image in numpy\n","            if pb:\n","                pred = frozen_func(x=tf.constant(imn)).numpy()\n","            elif saved_model:\n","                pred = model(imn, training=False).numpy()\n","            elif tflite:\n","                if int8:\n","                    scale, zero_point = input_details[0]['quantization']\n","                    imn = (imn / scale + zero_point).astype(np.uint8)  # de-scale\n","                interpreter.set_tensor(input_details[0]['index'], imn)\n","                interpreter.invoke()\n","                pred = interpreter.get_tensor(output_details[0]['index'])\n","                if int8:\n","                    scale, zero_point = output_details[0]['quantization']\n","                    pred = (pred.astype(np.float32) - zero_point) * scale  # re-scale\n","            pred[..., 0] *= imgsz[1]  # x\n","            pred[..., 1] *= imgsz[0]  # y\n","            pred[..., 2] *= imgsz[1]  # w\n","            pred[..., 3] *= imgsz[0]  # h\n","            pred = torch.tensor(pred)\n","        t3 = time_sync()\n","        dt[1] += t3 - t2\n","\n","        # NMS\n","        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n","        dt[2] += time_sync() - t3\n","\n","        # Second-stage classifier (optional)\n","        if classify:\n","            pred = apply_classifier(pred, modelc, img, im0s)\n","\n","        # Process predictions\n","        for i, det in enumerate(pred):  # per image\n","            seen += 1\n","            if webcam:  # batch_size >= 1\n","                p, s, im0, frame = path[i], f'{i}: ', im0s[i].copy(), dataset.count\n","            else:\n","                p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)\n","\n","            p = Path(p)  # to Path\n","            save_path = str(save_dir / p.name)  # img.jpg\n","            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n","            s += '%gx%g ' % img.shape[2:]  # print string\n","            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n","            imc = im0.copy() if save_crop else im0  # for save_crop\n","            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n","            if len(det):\n","                # Rescale boxes from img_size to im0 size\n","                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n","\n","                # Print results\n","                for c in det[:, -1].unique():\n","                    n = (det[:, -1] == c).sum()  # detections per class\n","                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n","\n","                # Write results\n","                for *xyxy, conf, cls in reversed(det):\n","                    if save_txt:  # Write to file\n","                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n","                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n","                        with open(txt_path + '.txt', 'a') as f:\n","                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n","\n","                    if save_img or save_crop or view_img:  # Add bbox to image\n","                        c = int(cls)  # integer class\n","                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n","                        annotator.box_label(xyxy, label, color=colors(c, True))\n","                        if save_crop:\n","                            save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n","\n","            # Print time (inference-only)\n","            print(f'{s}Done. ({t3 - t2:.3f}s)')\n","\n","            # Stream results\n","            im0 = annotator.result()\n","            if view_img:\n","                cv2.imshow(str(p), im0)\n","                cv2.waitKey(1)  # 1 millisecond\n","\n","            # Save results (image with detections)\n","            if save_img:\n","                if dataset.mode == 'image':\n","                    cv2.imwrite(save_path, im0)\n","                else:  # 'video' or 'stream'\n","                    if vid_path[i] != save_path:  # new video\n","                        vid_path[i] = save_path\n","                        if isinstance(vid_writer[i], cv2.VideoWriter):\n","                            vid_writer[i].release()  # release previous video writer\n","                        if vid_cap:  # video\n","                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n","                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","                        else:  # stream\n","                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n","                            save_path += '.mp4'\n","                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n","                    vid_writer[i].write(im0)\n","\n","    # Print results\n","    t = tuple(x / seen * 1E3 for x in dt)  # speeds per image\n","    print(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)\n","    if save_txt or save_img:\n","        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n","        print(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n","    if update:\n","        strip_optimizer(weights)  # update model (to fix SourceChangeWarning)\n","\n","\n","def parse_opt():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')\n","    parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob, 0 for webcam')\n","    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n","    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\n","    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n","    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')\n","    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n","    parser.add_argument('--view-img', action='store_true', help='show results')\n","    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n","    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n","    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')\n","    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')\n","    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')\n","    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n","    parser.add_argument('--augment', action='store_true', help='augmented inference')\n","    parser.add_argument('--visualize', action='store_true', help='visualize features')\n","    parser.add_argument('--update', action='store_true', help='update all models')\n","    parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')\n","    parser.add_argument('--name', default='exp', help='save results to project/name')\n","    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n","    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')\n","    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')\n","    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')\n","    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n","    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')\n","    opt = parser.parse_args()\n","    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n","    print_args(FILE.stem, opt)\n","    return opt\n","\n","\n","def main(opt):\n","    check_requirements(exclude=('tensorboard', 'thop'))\n","    run(**vars(opt))\n","\n","\n","if __name__ == \"__main__\":\n","    opt = parse_opt()\n","    main(opt)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Azu80w6avpc"},"source":["# Montar Drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0O_aR-JC3FpF","executionInfo":{"elapsed":36421,"status":"ok","timestamp":1635201264484,"user":{"displayName":"Gabriel Jacinto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj6BKo-nYPUdItRUFKM9XZ5PrjecgJ6GTIWSxuOw=s64","userId":"03470155761456273099"},"user_tz":180},"outputId":"d32603ee-01cf-4f04-f875-1a7b1e7a2265"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"UXMWr8bMa1Hy"},"source":["# Imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-vPN-rrng8I1","executionInfo":{"elapsed":13279,"status":"ok","timestamp":1635201467235,"user":{"displayName":"Gabriel Jacinto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj6BKo-nYPUdItRUFKM9XZ5PrjecgJ6GTIWSxuOw=s64","userId":"03470155761456273099"},"user_tz":180},"outputId":"b1766006-15c4-4938-c50a-fa6b53fd911e"},"source":["!pip install cvzone\n","!pip install mediapipe"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting cvzone\n","  Downloading cvzone-1.5.2.tar.gz (12 kB)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from cvzone) (4.1.2.30)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from cvzone) (1.19.5)\n","Building wheels for collected packages: cvzone\n","  Building wheel for cvzone (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cvzone: filename=cvzone-1.5.2-py3-none-any.whl size=18356 sha256=562d8862db53f7c22ee79f55de890d3e472d6e2f5b5bb23bf046ab67aed8aa62\n","  Stored in directory: /root/.cache/pip/wheels/5b/97/a1/601bf3f23368b10a67550cb6154cec1717eb03e5ea17ebdb7b\n","Successfully built cvzone\n","Installing collected packages: cvzone\n","Successfully installed cvzone-1.5.2\n","Collecting mediapipe\n","  Downloading mediapipe-0.8.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.0 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34.0 MB 51 kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.19.5)\n","Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from mediapipe) (0.37.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.2.0)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (0.12.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.3.2)\n","Installing collected packages: mediapipe\n","Successfully installed mediapipe-0.8.8.1\n"]}]},{"cell_type":"code","metadata":{"id":"9ihmeSD3W2pK"},"source":["import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import codecs\n","import cvzone\n","import os\n","from cvzone.SelfiSegmentationModule import SelfiSegmentation\n","from pathlib import Path\n","import sys\n","from google.colab.patches import cv2_imshow\n","path = '/content/drive/MyDrive/DPVSA/YOLOv5/ColorDetection/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uyy3hvC4gm3Z"},"source":["# Class Dict\n"]},{"cell_type":"code","metadata":{"id":"k-CMm1f4gnVK"},"source":["clasDict = {\n","  \"referee\": 0,\n","  \"player_team_2\": 1,\n","  \"goalkeeper_team_2\": 2,\n","  \"outsider\": 3,\n","  \"player_team_1\": 4,\n","  \"goalkeeper_team_1\": 5,\n","  \"ball\": 6\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LBeyrvYw2lDB"},"source":["# Color Detection"]},{"cell_type":"code","metadata":{"id":"BN53Wid02lDL"},"source":["def cut_box(image, coordinates, cropCoordinates):\n","\n","  col1 = coordinates[0]\n","  row1 = coordinates[1]\n","  col2 = coordinates[2]\n","  row2 = coordinates[3]\n","  box = image[row1:row2,col1:col2]\n","\n","  col1 = cropCoordinates[0]\n","  row1 = cropCoordinates[1]\n","  col2 = cropCoordinates[2]\n","  row2 = cropCoordinates[3]\n","  cropBox = image[row1:row2,col1:col2]\n","\n","  #cv2_imshow(box)\n","  #cv2_imshow(cropBox)\n","\n","  return box, cropBox"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uje_tEVt2lDP"},"source":["def color_percentage(colorPixels,coordinates = 0, totalPixels = 0, mode = 0):\n","\n","  if mode == 0: # no background mode\n","    percentage = colorPixels/totalPixels\n","  else: # background mode\n","    width = coordinates[0] - coordinates[2]\n","    height = coordinates[1] - coordinates[3]\n","    percentage = colorPixels/(width*height)\n","\n","  return percentage"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UJcE9CFf2lDM"},"source":["def color_detection(image, coordinates, cropped = False):\n","\n","  # TODO REVISAR CORES\n","  hsvImage = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","  colorCount = {\n","      'red':0,\n","      'blue':0,\n","      'yellow':0,\n","      'green':0,\n","      'white':0,\n","      'black':0\n","  }\n","\n","\n","  # BACKGROUND SEGMENTATION MASK\n","  pureBlackRange = np.array([[0,0,0],[0,0,0]])\n","  pureBlackMask = cv2.inRange(hsvImage,pureBlackRange[0],pureBlackRange[1])\n","  pureBlackMask = np.where(pureBlackMask == 255, 1, pureBlackMask)\n","  pureBlackMask = np.where(pureBlackMask == 0, 255, pureBlackMask) \n","  pureBlackMask = np.where(pureBlackMask == 1, 0, pureBlackMask)\n","  notBackground = np.count_nonzero(pureBlackMask == 255)\n","\n","  if(cropped):\n","    notBackground = image.shape[0]*image.shape[1]\n","    \n","\n","  #print('PURE BLACK MASK')   \n","  #cv2_imshow(pureBlackMask)\n","\n","  # RED MASK  \n","  redRange1 = np.array([[0,70,50],[20,255,255]])\n","  redRange2 = np.array([[170,70,50],[180,255,255]])\n","  maskRed1 = cv2.inRange(hsvImage,redRange1[0],redRange1[1])\n","  maskRed2= cv2.inRange(hsvImage,redRange2[0],redRange2[1])\n","  maskRed3 = cv2.bitwise_or(maskRed1, maskRed2)\n","  maskRed4 = cv2.bitwise_and(maskRed3, pureBlackMask)\n","  redOcurrence = np.count_nonzero(maskRed4 == 255)\n","  redPerc = color_percentage(redOcurrence, totalPixels = notBackground, mode = 0 )\n","  colorCount['red'] = redPerc\n","  #print('redPerc',redPerc)\n","\n","  # WHITE MASK\n","  whiteRange = np.array([[0,0,170],[179,38,255]])\n","  maskWhite1 = cv2.inRange(hsvImage,whiteRange[0],whiteRange[1])\n","  maskWhite2 = cv2.bitwise_and(maskWhite1, pureBlackMask)\n","  whiteOcurrence = np.count_nonzero(maskWhite2 == 255)\n","  whitePerc = color_percentage(whiteOcurrence, totalPixels = notBackground, mode = 0 )\n","  colorCount['white'] = whitePerc\n","  #print('whitePerc',whitePerc)\n","\n","  # GREEN MASK\n","  greenRange = np.array([[37,66,102],[81,255,255]])\n","  maskGreen1 = cv2.inRange(hsvImage,greenRange[0],greenRange[1])\n","  maskGreen2 = cv2.bitwise_and(maskGreen1, pureBlackMask)\n","  greenOcurrence = np.count_nonzero(maskGreen2 == 255)\n","  greenPerc =  color_percentage(greenOcurrence, totalPixels = notBackground, mode = 0)\n","  colorCount['green'] = greenPerc\n","  #print('greenPerc',greenPerc)\n","\n","  #BLUE MASK\n","  blueRange = np.array([[85,70,50],[140,255,255]])\n","  maskBlue1 = cv2.inRange(hsvImage,blueRange[0],blueRange[1])\n","  maskBlue2 =  cv2.bitwise_and(maskBlue1, pureBlackMask)\n","  blueOcurrence = np.count_nonzero(maskBlue2 == 255)\n","  bluePerc = color_percentage(blueOcurrence, totalPixels = notBackground, mode = 0)\n","  colorCount['blue'] = bluePerc\n","  #print('BluePerc',bluePerc)\n","\n","\n","  # YELLOW MASK\n","  yellowRange = np.array([[23,38,102],[33,255,255]])\n","  maskYellow1 = cv2.inRange(hsvImage,yellowRange[0],yellowRange[1])\n","  maskYellow2 = cv2.bitwise_and(maskYellow1, pureBlackMask)\n","  yellowOcurrence = np.count_nonzero(maskYellow2 == 255)\n","  yellowPerc  = color_percentage(yellowOcurrence, totalPixels = notBackground, mode = 0)\n","  colorCount['yellow'] = yellowPerc\n","  #print('yellowPerc', yellowPerc)\n","\n","  # BLACK MASK\n","  blackRange = np.array([[0,0,0],[255,70,50]])\n","  maskBlack1 = cv2.inRange(hsvImage,blackRange[0],blackRange[1])\n","  maskBlack2 = cv2.bitwise_and(maskBlack1, pureBlackMask)\n","  blackOcurrence = np.count_nonzero(maskBlack2 == 255)\n","  blackPerc = color_percentage(blackOcurrence, totalPixels = notBackground, mode = 0)\n","  colorCount['black'] = blackPerc\n","  #print('blackPerc',blackPerc)\n","\n","  color = max(colorCount,key=colorCount.get) \n","  if(color == 'green'):\n","    colorCount.pop(color, None)\n","    secondColor = max(colorCount,key=colorCount.get)\n","    if colorCount[secondColor] > 0.1:\n","      color = secondColor\n","    else:\n","      colorCount['green'] = greenPerc\n","      color = max(colorCount,key=colorCount.get) \n","\n","\n","  colorPercentage = colorCount[color]\n","  return color,colorPercentage"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Pb1R4-kustp"},"source":["def color_recognition(orImage, segImage, cropImage, coordinates, cropCoordinates):\n","\n","  backgroundPixels = np.count_nonzero(segImage == 0) / 3\n","  backgroundPerc = backgroundPixels / (orImage.shape[0] * orImage.shape[1])\n","  #print(backgroundPerc)\n","\n","  if(backgroundPerc > 0.9):\n","    #print('PRETEOU')\n","    color, percentage = color_detection(cropImage, cropCoordinates, cropped=True)\n","  else:\n","    #print('NAO PRETEOU')\n","    color, percentage = color_detection(segImage, coordinates)\n","\n","  return color, percentage"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1rFIqOA1Ttz7"},"source":["def pixel_position(boxplot, imWidth, imHeight):\n","\n","  boxClass = boxplot[0]\n","  xCenter = boxplot[1]\n","  yCenter = boxplot[2]\n","  width = boxplot[3]\n","  height = boxplot[4]\n","\n","  xTopLeft = int((xCenter - width*0.5) * imWidth) \n","  yTopLeft = int((yCenter - height*0.5) * imHeight) \n","  xBottomRight = int((xCenter + (width*0.5))*imWidth)\n","  yBottomRight = int((yCenter + (height*0.5))*imHeight) \n","\n","  coordinates = [xTopLeft, yTopLeft, xBottomRight, yBottomRight]\n","\n","  # Cropped Coordinates\n","\n","  croppedWidth = boxplot[3]*0.5\n","  croppedHeight = boxplot[4]*0.7\n","  \n","  cropXTopLeft = int((xCenter - croppedWidth*0.5) * imWidth) \n","  cropYTopLeft = int((yCenter - croppedHeight*0.5) * imHeight) \n","  cropXBottomRight = int((xCenter + (width*0.5))*imWidth)\n","  CropYBottomRight = int((yCenter + (height*0.5))*imHeight) -10\n","\n","  cropCoordinates = [cropXTopLeft, cropYTopLeft, cropXBottomRight, CropYBottomRight]\n","\n","  return boxClass, coordinates, cropCoordinates"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nuVzOKv7mtjG"},"source":["def color_inference(image,labels):\n","  labels = np.loadtxt(path+'21062018_55410.txt')\n","  image = cv2.imread(path+'21062018_55410.png')\n","  cv2_imshow(image)\n","  segmentor = SelfiSegmentation()\n","  for row in labels:\n","    if row[0] == 4 :\n","      boxClass, coordinates, cropCoordinates = pixel_position(row,image.shape[1], image.shape[0])\n","      box, cropBox = cut_box(image, coordinates, cropCoordinates)\n","      segmentedBox = segmentor.removeBG(box,(0,0,0),threshold=0.15)\n","      #segmentedBox2 = segmentor.removeBG(box,(0,0,0),threshold=0.5)\n","      color, percentage = color_recognition(box, segmentedBox, cropBox, coordinates, cropCoordinates)\n","      print(color, percentage)\n","      cv2_imshow(box)\n","      #cv2_imshow(segmentedBox)\n","      #cv2_imshow(segmentedBox2)\n","      cv2_imshow(cropBox)\n","      print(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ytDagv12lDQ"},"source":["def detection(image, labels, show_crop=False):\n","  camisas = {}\n","  posicao_camisa = {}\n","  segmentor = SelfiSegmentation()\n","  i=0\n","  for row in labels:\n","    if(row[0]==4) or (row[0]==1):\n","      norm_cordinates = [row[1],row[2],row[3],row[4]]\n","      boxClass, coordinates, cropCoordinates = pixel_position(row, image.shape[1], image.shape[0])\n","      box, cropBox = cut_box(image, coordinates, cropCoordinates)\n","      segmentedBox = segmentor.removeBG(box,(0,0,0),threshold=0.1)\n","      finalColor, percentage = color_recognition(box, segmentedBox, cropBox, coordinates, cropCoordinates)\n","      posicao_camisa[\"linha {}\".format(i)] = [norm_cordinates,coordinates,finalColor]\n","      i+=1\n","      #LÃ³gica de contagem das cores das camisas\n","      if finalColor not in camisas.keys():\n","        camisas[finalColor] = 1\n","      else:\n","        camisas[finalColor] += 1\n","      \n","      #Mostra as informaÃ§Ãµes para debug\n","      if show_crop:\n","        print(percentage)\n","        print(finalColor)\n","        cv2_imshow(box)\n","        #cv2_imshow(segmentedBox)\n","        #cv2.rectangle(image, (xTopLeft, yTopLeft), (xBottomRight, yBottomRight), (255, 0, 0), 2)\n","        #row[0] = 666\n","  return camisas, posicao_camisa\n","#cores = detection(image,labels)\n","#cv2_imshow(image)\n","#[cor,pt1,pt2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6gqalzhlEnyo","executionInfo":{"status":"ok","timestamp":1635216682694,"user_tz":180,"elapsed":451,"user":{"displayName":"Gabriel Jacinto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj6BKo-nYPUdItRUFKM9XZ5PrjecgJ6GTIWSxuOw=s64","userId":"03470155761456273099"}}},"source":["def altera_labels(labels_file, labels_file_outuput, img, camisas_cores, posicao_bbox):\n","  escolha_time = camisas_cores.copy()\n","  color_time_1_c4 = max(escolha_time, key=escolha_time.get)\n","  #{'yellow': 10, 'white': 5, 'red': 3, 'green': 2}\n","  escolha_time.pop(color_time_1_c4, None)\n","  color_time_2_c1 = max(escolha_time, key=escolha_time.get)\n","  \n","  teste = labels_file\n","  #Escrita das classes e conversÃ£o dos valores\n","  with open(labels_file, \"r\") as f:\n","    with open(labels_file_outuput, \"w\") as d:\n","      i = 0\n","      for line in f:\n","        if (line.startswith(\"4\")) or (line.startswith(\"5\")):\n","          num_linha = \"linha {}\".format(i)\n","          #a = str(line.split()[1])\n","          #b = str(line.split()[2])\n","          #d.write(\"{} {}\\n\".format(posicao_bbox[num_linha][0][0] == float(a), posicao_bbox[num_linha][0][1] == float(b)))\n","          conf = line.split()[-1]\n","          if posicao_bbox[num_linha][0][0] == float(line.split()[1]) and posicao_bbox[num_linha][0][1] == float(line.split()[2]):\n","            if posicao_bbox[num_linha][2] == color_time_1_c4:\n","              nova_classe = 4\n","              d.write(\"{} {} {} {} {} {}\\n\".format(posicao_bbox[num_linha][1][0],posicao_bbox[num_linha][1][1],posicao_bbox[num_linha][1][2],posicao_bbox[num_linha][1][3],conf,nova_classe))\n","            elif posicao_bbox[num_linha][2] == color_time_2_c1:\n","              nova_classe = 1\n","              d.write(\"{} {} {} {} {} {}\\n\".format(posicao_bbox[num_linha][1][0],posicao_bbox[num_linha][1][1],posicao_bbox[num_linha][1][2],posicao_bbox[num_linha][1][3],conf,nova_classe))\n","            else: #Caso a cor detectada seja diferente das dos dois times, decidimos por quantidade de jogadores \n","              if camisas_cores[color_time_1_c4]>=camisas_cores[color_time_2_c1]:\n","                nova_classe = 1\n","                d.write(\"{} {} {} {} {} {}\\n\".format(posicao_bbox[num_linha][1][0],posicao_bbox[num_linha][1][1],posicao_bbox[num_linha][1][2],posicao_bbox[num_linha][1][3],conf,nova_classe))\n","              if camisas_cores[color_time_1_c4]<camisas_cores[color_time_2_c1]:\n","                nova_classe = 4\n","                d.write(\"{} {} {} {} {} {}\\n\".format(posicao_bbox[num_linha][1][0], posicao_bbox[num_linha][1][1],posicao_bbox[num_linha][1][2],posicao_bbox[num_linha][1][3],conf,nova_classe))\n","          i += 1\n","        else:\n","          image = cv2.imread(img)\n","          boxplot = line.split()\n","          \n","          conf = float(boxplot[-1])\n","          boxClass = boxplot[0]\n","          xCenter = float(boxplot[1])\n","          yCenter = float(boxplot[2])\n","          width = float(boxplot[3])\n","          height = float(boxplot[4])\n","\n","          xTopLeft = int((xCenter - width*0.5) *  image.shape[1]) \n","          yTopLeft = int((yCenter - height*0.5) * image.shape[0]) \n","          xBottomRight = int((xCenter + (width*0.5))* image.shape[1])\n","          yBottomRight = int((yCenter + (height*0.5))*image.shape[0])\n","          \n","          #boxClass, new_coordinates,_ = pixel_position(label, image.shape[1], image.shape[0])\n","          d.write(\"{} {} {} {} {} {}\\n\".format(xTopLeft, yTopLeft,xBottomRight,  yBottomRight, conf, boxClass))\n","\n","def count_files(folder):\n","  print(\"folder has \" + str(len(os.listdir(folder))) + \" files\")"],"execution_count":160,"outputs":[]},{"cell_type":"code","metadata":{"id":"RYAwwdNhtZok","executionInfo":{"status":"ok","timestamp":1635216687758,"user_tz":180,"elapsed":439,"user":{"displayName":"Gabriel Jacinto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj6BKo-nYPUdItRUFKM9XZ5PrjecgJ6GTIWSxuOw=s64","userId":"03470155761456273099"}}},"source":["img_list=[img for img in sorted(Path('/content/drive/MyDrive/DPVSA/YOLOv5/ColorDetection/teste/images_video').glob('*.png'))]\n","label_list=[label for label in sorted(Path('/content/drive/MyDrive/DPVSA/YOLOv5/ColorDetection/teste/labels_video').glob('*.txt'))]\n","label_converted_list=[label for label in sorted(Path('/content/drive/MyDrive/DPVSA/YOLOv5/ColorDetection/teste/labels_convertidas').glob('*.txt'))] #os arquivos serÃ£o alterados aqui"],"execution_count":161,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFzeldt8JYPe"},"source":["\"\"\"def define_camisas(todas_camisas_video):\n","  camisas = todas_camisas_video\n","  camisa_time1 = max(camisas, key=camisas.get)\n","  camisas.pop(camisa_time1, None) \n","  camisa_time2 = max(camisas, key=camisas.get)\n","  return camisa_time1, camisa_time2\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CLagYgijJWGF"},"source":["\"\"\"cores_totais = {}\n","for img,label in zip(img_list,label_list): #loop detecÃ§Ã£o de cor\n","  labels2read = np.loadtxt(str(label))\n","  image = cv2.imread(str(img))\n","  cores,posicao_camisa = detection(image,labels2read,show_crop=True)\n","  cv2_imshow(image)\n","  print(\"{}: {}\".format(str(img)[57:],cores))\n","  for cor in cores.keys():\n","    if cor in cores_totais.keys():\n","      cores_totais[cor] += cores[cor]\n","    else: \n","      cores_totais[cor] = cores[cor]\n","\n","cor_time1, cor_time2 = define_camisas(cores_totais)\n","for img,label,converted_label in zip(img_list,label_list,label_converted_list): #loop alteraÃ§Ã£o de classes\n","  label_path = str(label)\n","  converted_label_path = str(converted_label)\n","  img_path = str(img)\n","  altera_labels(label_path, converted_label_path, img_path, cor_time1, cor_time2, posicao_camisa)\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AeDu96ZdtlA4","executionInfo":{"status":"ok","timestamp":1635216692368,"user_tz":180,"elapsed":451,"user":{"displayName":"Gabriel Jacinto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj6BKo-nYPUdItRUFKM9XZ5PrjecgJ6GTIWSxuOw=s64","userId":"03470155761456273099"}}},"source":["for img,label,converted_label in zip(img_list,label_list,label_converted_list):\n","  labels = np.loadtxt(str(label))\n","  image = cv2.imread(str(img))\n","  cores,posicao_camisa = detection(image,labels)#,show_crop=True)\n","  #cv2_imshow(image)\n","  print(\"{}: {}\".format(str(img)[57:],cores))\n","  label_path = str(label)\n","  converted_label_path = str(converted_label)\n","  img_path = str(img)\n","  altera_labels(label_path, converted_label_path, img_path, cores, posicao_camisa)"],"execution_count":162,"outputs":[]},{"cell_type":"code","metadata":{"id":"0BDOLN7IMLuv","executionInfo":{"status":"ok","timestamp":1635216696047,"user_tz":180,"elapsed":460,"user":{"displayName":"Gabriel Jacinto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj6BKo-nYPUdItRUFKM9XZ5PrjecgJ6GTIWSxuOw=s64","userId":"03470155761456273099"}}},"source":["def teste(coordinates):\n","\n","  x1 = coordinates[0]\n","  y1 = coordinates[1]\n","  x2 = coordinates[2]\n","  y2 = coordinates[3]\n","  classe = coordinates[5]\n","  start = (x1,y1)\n","  finish = (x2,y2)\n","\n","  #cv2_imshow(box)\n","  #cv2_imshow(cropBox)\n","\n","  return start, finish, classe"],"execution_count":163,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Kr2iwELN5lc","executionInfo":{"status":"ok","timestamp":1635216698150,"user_tz":180,"elapsed":569,"user":{"displayName":"Gabriel Jacinto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj6BKo-nYPUdItRUFKM9XZ5PrjecgJ6GTIWSxuOw=s64","userId":"03470155761456273099"}}},"source":["for img,converted_label in zip(img_list,label_converted_list):\n","  image = cv2.imread(str(img))\n","  file = np.loadtxt(str(converted_label))\n","  cores = {\n","    0: (255, 0, 0), #\"referee\" --red\n","    1: (0,0,0), #\"player_team_2\" --balck\n","    2: (0,255,0), #\"goalkeeper_team_2\" --green\n","    3: (255,255,0), #\"outsider\" --yellow\n","    4: (255,255,255), #\"player_team_1\" --white\n","    5: (255,0,255), #\"goalkeeper_team_1\" --pink\n","    6: (0,0,255)} #\"ball\" --blue\n","  for row in file:\n","    x1 = int(row[0])\n","    y1 = int(row[1])\n","    x2 = int(row[2])\n","    y2 = int(row[3])\n","    classe = int(row[5])\n","    color = cores[classe]\n","    #print(type(x1))\n","    cv2.rectangle(image, (x1,y1), (x2,y2), color)\n","  cv2_imshow(image) "],"execution_count":164,"outputs":[]},{"cell_type":"code","metadata":{"id":"6y6z-1SEam9s","executionInfo":{"status":"ok","timestamp":1635216319446,"user_tz":180,"elapsed":989,"user":{"displayName":"Gabriel Jacinto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj6BKo-nYPUdItRUFKM9XZ5PrjecgJ6GTIWSxuOw=s64","userId":"03470155761456273099"}}},"source":["cap = cv2.VideoCapture(\"/content/drive/MyDrive/DPVSA/video_teste.mp4\")\n","i = 0\n","#os.chdir(\"/content/drive/MyDrive/DPVSA/YOLOv5/ColorDetection/teste/images_video\")\n","while(cap.isOpened()):\n","    ret, frame = cap.read()\n","     \n","    # This condition prevents from infinite looping\n","    # incase video ends.\n","    if i == 10:\n","      break\n","    if ret == False:\n","      break\n","     \n","    # Save Frame by Frame into disk using imwrite method\n","    cv2.imwrite('/content/drive/MyDrive/DPVSA/YOLOv5/ColorDetection/teste/images_video/CÃ³pia de video_teste_{}.jpg'.format(i+1), frame)\n","  \n","    i += 1\n"," \n","cap.release()\n","cv2.destroyAllWindows()"],"execution_count":150,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XiqJ8QXkUVmO"},"source":["## OUTPUT\n","\n","The output should be a list of bounding box arrays with the format (x1, y1, x2, y2, conf, class) for each frame.\n","\n","x1, y1: Top left\n","\n","x2, y2: Bottom Right "]},{"cell_type":"code","metadata":{"id":"epjiC-MYD4AH"},"source":["result_frame_N = [(397.5460122699385, 771.5521472392636, 458.89570552147234, 938.4233128834355, 0.8123, 3.0), ...]"],"execution_count":null,"outputs":[]}]}